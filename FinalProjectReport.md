# 期末專案報告：自主倫理推理與道德決策 AI 系統

## 摘要

本專案旨在探討一項未來 AI 的突破性能力：**自主倫理推理與道德決策**。我們預期，到 2045 年，AI 將從單純的決策工具升級為具備「價值推理」能力的社會參與者。本報告闡述了該能力的具體內容、實現所需的關鍵「成分」，確定了所需的多重機器學習範式，並設計與實作了一個可行的「道德傾向預測」簡化模型問題。

---

## 一、 AI 的未來能力：自主倫理推理與道德決策系統

### 具體能力內容與應用

我們認為，20 年後 AI 最重要的突破將是能夠在道德模糊或價值衝突的情境中，運用結構化的倫理框架（如功利主義、義務論）進行推理，並對其決策提供清晰、可被人類理解與接受的「**道德證明**」。

這項能力的核心在於 AI 能夠：

* **情境解析與價值識別：** 快速識別情境中的所有潛在利害關係人、評估其生命、自由或財產等權重，並識別決策涉及的倫理原則（如公平性、不可侵犯性）。
* **多框架推演：** 能夠模擬不同道德哲學框架下的最佳行動方案，並評估每種方案的長期和短期社會後果。
* **生成道德證明：** 不僅輸出「決定 A」，還能輸出「因為我們依據義務論原則，優先保障行為的不可侵犯性，儘管這會犧牲較少數人的利益」這樣的推理路徑。

### 應用場景：AI 醫療資源分配與倫理審查

* **醫療分配（高道德複雜性）：** 在災難或資源極度稀缺時（如呼吸器分配），AI 不僅依賴硬性的醫療指標，還能將社會價值（如個體貢獻、家庭責任、年齡潛力）納入考慮，並在倫理委員會的指導下，生成一份權衡所有爭議點的資源分配報告與決策理由。
* **公共政策與倫理審查：** 作為政府或大型機構的「價值思考助理」，AI 可在制定新法律或公共政策前，模擬該政策可能對弱勢群體或特定文化產生的倫理衝擊，幫助人類發現隱藏的偏見和道德盲點。

---

## 二、 所需的成分與資源

實現倫理推理 AI 系統需要一個高度整合的複合架構，結合了豐富的資料、多樣的計算工具以及複雜的學習範式。

| 成分類別 | 具體資源與技術 | 扮演角色與依存性 |
| :--- | :--- | :--- |
| **資料** | 1. **道德兩難語料庫 (MD-Corpus)**： 結構化情境與人類回饋（決策、情緒、文字解釋）。2. **規範性文本知識庫**： 大規模、經過語義解析的法律條文、醫學倫理規範、哲學經典等。3. **多模態環境數據**： 即時感測器數據、視覺影像，用於精準捕捉情境細節。 | 作為 AI 的「道德教科書」。MD-Corpus 用於初始化策略模型；規範文本用於 Symbolic Reasoning Engine 進行邏輯校驗。 |
| **工具** | 1. **圖神經網路 (GNN)**： 用於建模複雜的利害關係人網路及其因果關係。2. **符號推理引擎 (Symbolic Reasoning)**： 用於執行高層次的倫理規則檢查（e.g., 非直接傷害原則），確保決策的邏輯一致性。3. **大型語言模型 (LLMs)**： 負責將 GNN 和 Symbolic Engine 的結果，轉譯成具備高道德說服力的自然語言證明。 | GNN 處理複雜關係；Symbolic Engine 確保合規性；LLM 負責透明度輸出。三者結果透過 RL 的獎勵函數整合。 |
| **硬體與環境** | 專為多模態即時推理優化的邊緣運算硬體（Edge AI Chipsets）；具備高頻寬、低延遲的感測器套件（如自動駕駛車）。 | 實現即時決策的基礎，特別是在時間壓力下的緊急情況（如幾毫秒內的避險）。 |
| **學習架構** | 強化學習結合人類回饋 (RLHF) 與自我監督學習 (SSL) 的混合範式。 | SSL 對規範文本進行預訓練，建立倫理概念的內隱表示；RLHF 負責將策略模型的輸出與人類（或倫理專家）的偏好對齊。 |

---

## 三、 涉及的機器學習類型：混合範式 

該能力的實現主要涉及**強化學習 (RL)**，並深度結合**監督式學習 (SL)** 和**符號推理 (SR)**。

### 為什麼需要 RL？

道德決策本質上是序列性的、動態的策略問題。在自動駕駛情境中，AI 必須根據連續不斷的環境觀測採取行動，並在一個時間步長上最大化或最小化一個複雜的「道德效用」函數。

### 目標訊號（獎勵函數）：

RL 的獎勵訊號是一個複雜的**多目標函數**：

* **道德一致性獎勵：** 來自 Symbolic Engine，如果決策違反了不可妥協的倫理規則（如禁止主動傷害），則給予巨大的負面懲罰。
* **結果效用獎勵：** 衡量行動結果在功利角度下的客觀效用（如最少傷亡人數）。
* **可解釋性/偏好獎勵：** 來自 RLHF，根據人類倫理專家對 AI 輸出的道德證明和決策路徑的評分。

### 資料來源與回饋：

資料來源是環境觀測（多模態數據）和道德語料庫（SL 初始化），而目標訊號則是來自於複雜的內部審查機制（Symbolic Engine）和外部的人類反饋（RLHF）。

---

## 四、 第一步的「可實作模型問題」

為了驗證 AI 學習人類道德傾向的可行性，我們設計並實作一個極簡化的「道德傾向預測模型」。

### 問題設計：道德框架傾向預測

**目標：** 在高度結構化的自動駕駛危機情境中，預測人類傾向於採取哪一種主流道德框架的決策。這個問題將複雜的「推理」降維為可訓練的「分類」。

* **輸入 (Input Features)：** 四個結構化的、可量化的情境變量 $P = (P_1, P_2, P_3, P_4)$。
    * $P_1$: 避讓方案 A 導致的傷亡人數（整數）。
    * $P_2$: 乘客生命權重（0: 一般；1: 高權重，如兒童）。
    * $P_3$: 行為意圖（0: 主動干預/轉向；1: 被動不作為/直行）。
    * $P_4$: 潛在法律風險分數（0-10）。
* **輸出 (Target)：** 二元分類 $\hat{Y} \in \{0, 1\}$。
    * $0$: 功利主義傾向（犧牲少數救多數）。
    * $1$: 義務論傾向（避免主動干預或傷害）。
* **任務目標：** 最大化分類準確度。

### 模型與方法：羅吉斯迴歸 (Logistic Regression)

我們選擇羅吉斯迴歸作為簡化模型：

* **可解釋性：** 羅吉斯迴歸能清晰地顯示每個輸入特徵（$P_i$）對最終道德傾向的權重或影響方向，有助於我們理解人類決策的核心驅動因素。
* **計算效率：** 對於四個特徵的二元分類，它是最簡單、最穩定的選擇，符合「第一步」驗證概念的要求。

### 實作與結果（請參考 `FinalProjectProgramming.py` 程式碼）

根據實際模擬結果，模型在測試集上達到了 **79.25%** 的分類準確度。模型權重分析（Log Odds 係數）揭示了以下關鍵洞察：

| 特徵 | 係數 | 影響方向與啟示 |
| :--- | :--- | :--- |
| **$P_1$ (傷亡人數)** | **-0.943** (負向係數) | 數字越多，越傾向功利主義 ($Y=0$)。 |
| **$P_2$ (乘客權重)** | **+1.244** (正向係數) | 權重越高，越傾向義務論 ($Y=1$)。這是最強驅動因素。 |
| **$P_3$ (行為意圖/不作為)** | **+0.695** (正向係數) | 不作為越傾向義務論 ($Y=1$)。 |
| **$P_4$ (法律風險)** | **+0.485** (正向係數) | 風險越高，越傾向義務論 ($Y=1$)。 |

範例預測也符合直覺：在「大量受害者」情境中，AI 預測傾向為功利主義（95.8% 機率）；而在「高權重乘客且被動不作為」的情境中，則傾向義務論（99.3% 機率）。

---

## 討論：簡化模型的啟示與未來挑戰

這個實作的結果（79.25% 準確度）給予我們兩大啟示：

1.  **道德模式的可學習性與模糊性：** **79.25% 的準確度**證實了道德判斷的傾向性在結構化後是可被捕捉和預測的，驗證了核心概念。然而，未達完美的準確度（如 100%）也真實反映了**人類道德判斷本身的模糊性與個體差異**，即便在高度結構化的情境中，仍有約 20% 的情況無法僅憑這四個簡單特徵進行確定分類。這證明了未來 AI 必須具備處理不確定性和模糊性的能力。
2.  **核心難題的揭示——價值權重的競爭與複雜性：** 透過模型權重分析，我們觀察到 **$P_2$（乘客生命權重，+1.244）是影響義務論傾向的最強驅動因素**，而 **$P_1$（潛在受害者數量，-0.943）是影響功利主義傾向的最強反向驅動因素**。這兩個特徵的係數最高，表明**「保護少數高權重者」與「拯救多數人」**這兩種競爭性價值之間的權衡，是決策的核心。這證明了未來 AI 必須透過複雜的強化學習獎勵函數來動態地權衡這些競爭性價值，這是 RL 系統設計中最困難的環節。
---

## 附錄

### Python程式輸出結果

![alt text](image.png)